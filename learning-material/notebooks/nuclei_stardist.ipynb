{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting cell nuclei with the `stardist` napari plugin\n",
    "\n",
    "Plugins are a way of extending Napari's functionality beyond the basic viewer. Many plugins are available for common analysis tasks such as segmentation and filtering. In this activity, we will segment nuclei using the [stardist napari plugin](https://github.com/stardist/stardist-napari). Please visit the [napari hub](https://www.napari-hub.org/) for a list of all available plugins."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stardist](../images/stardist_fig.png)\n",
    "# Cell nuclei detection\n",
    "---\n",
    "\n",
    "[StarDist](https://github.com/stardist/stardist) is a deep-learning based Python library used for segmenting star-convex objects, such as cell nuclei, in 2D and 3D images. It is also available as plugins for [ImageJ](https://imagej.net/plugins/stardist), [Napari](https://github.com/stardist/stardist-napari), and [Qupath](https://qupath.readthedocs.io/en/0.3/docs/advanced/stardist.html).\n",
    "\n",
    "In this notebook, we will use StarDist to detect cell nuclei in an image extracted from the [DeepSlides](https://zenodo.org/record/1184621) public dataset of histopathology images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Check that you have all the necessary packages installed, including `napari` and the `stardist-napari` plugin. If not, you can use the `!` symbol to install them directly from the Jupyter notebook (otherwise, you can use your terminal).\n",
    "\n",
    "```{note}\n",
    "Napari plugins are Python packages, so you can install them using `pip` (e.g. `pip install stardist-napari`)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 13:43:59.887705: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-16 13:43:59.889367: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 13:43:59.915096: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-16 13:43:59.915123: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-16 13:43:59.915141: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-16 13:43:59.920885: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-16 13:43:59.921762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 13:44:00.494753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "from stardist.models import StarDist2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "\n",
    "The image we'll use in this tutorial is available for download on [Zenodo](https://zenodo.org/record/8099852) (`deepslide.png`).\n",
    "\n",
    "In the cell below, we use a Python package called [pooch](https://pypi.org/project/pooch/) to automatically download the image from Zenodo into the **data** folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image deepslide.png into: /home/wittwer/code/napari-imaging-epfl-2023/learning-material/data\n"
     ]
    }
   ],
   "source": [
    "import pooch\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('.').resolve().parent / 'data'\n",
    "fname = 'deepslide.png'\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=\"https://zenodo.org/record/8099852/files/deepslide.png\",\n",
    "    known_hash=\"md5:67d2dac6f327e2d3749252d46799861a\",\n",
    "    path=data_path,\n",
    "    fname=fname,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(f'Downloaded image {fname} into: {data_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the image\n",
    "\n",
    "We use the `imread` function from Scikit-image to read our PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image in an array of shape: (513, 513, 3) and data type uint8\n",
      "Intensity range: [3 - 245]\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "image = imread(data_path / 'deepslide.png')\n",
    "\n",
    "print(f'Loaded image in an array of shape: {image.shape} and data type {image.dtype}')\n",
    "print(f'Intensity range: [{image.min()} - {image.max()}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into troubles, don't hesitate to ask for help ü§öüèΩ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image into Napari\n",
    "\n",
    "Let's open a viewer and load our image to have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'H&E (DeepSlides)' at 0x7f5af79d26a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image, name=\"H&E (DeepSlides)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity normalization\n",
    "\n",
    "Let's rescale our image to the range 0-1. By doing so, it is also converted to an array of data type `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity range: [0.0 - 1.0]\n",
      "Array type: float64\n"
     ]
    }
   ],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "image_normed = rescale_intensity(image, out_range=(0, 1))\n",
    "\n",
    "print(f'Intensity range: [{image_normed.min()} - {image_normed.max()}]')\n",
    "print(f'Array type: {image_normed.dtype}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a StarDist model\n",
    "\n",
    "The StarDist developers provide a few pre-trained models that may already be applied to suitable images.\n",
    "\n",
    "Here, we will use the *Versatile (H&E nuclei)* model that was trained on images from the MoNuSeg 2018 training data and the TNBC dataset from Naylor et al. (2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model '2D_versatile_he' for 'StarDist2D'.\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.692478, nms_thresh=0.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 13:44:02.966985: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-10-16 13:44:02.967020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: vilthuril\n",
      "2023-10-16 13:44:02.967025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: vilthuril\n",
      "2023-10-16 13:44:02.967210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.113.1\n",
      "2023-10-16 13:44:02.967235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.86.5\n",
      "2023-10-16 13:44:02.967241: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 535.86.5 does not match DSO version 535.113.1 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StarDist2D(2D_versatile_he): YXC ‚Üí YXC\n",
       "‚îú‚îÄ Directory: None\n",
       "‚îî‚îÄ Config2D(n_dim=2, axes='YXC', n_channel_in=3, n_channel_out=33, train_checkpoint='weights_best.h5', train_checkpoint_last='weights_last.h5', train_checkpoint_epoch='weights_now.h5', n_rays=32, grid=(2, 2), backbone='unet', n_classes=None, unet_n_depth=3, unet_kernel_size=[3, 3], unet_n_filter_base=32, unet_n_conv_per_depth=2, unet_pool=[2, 2], unet_activation='relu', unet_last_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_prefix='', net_conv_after_unet=128, net_input_shape=[None, None, 3], net_mask_shape=[None, None, 1], train_shape_completion=False, train_completion_crop=32, train_patch_size=[512, 512], train_background_reg=0.0001, train_foreground_only=0.9, train_sample_cache=True, train_dist_loss='mae', train_loss_weights=[1, 0.1], train_class_weights=(1, 1), train_epochs=200, train_steps_per_epoch=200, train_learning_rate=0.0003, train_batch_size=8, train_n_val_patches=3, train_tensorboard=True, train_reduce_lr={'factor': 0.5, 'patience': 50, 'min_delta': 0}, use_gpu=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StarDist2D.from_pretrained(\"2D_versatile_he\")\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "We use the `predict_instances` method of the model to generate a segmenation mask (`labels`) and a representation of the cell nuclei as polygons (`polys`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 cells detected.\n"
     ]
    }
   ],
   "source": [
    "labels, polys = model.predict_instances(\n",
    "    image_normed,  # The image must be normalized\n",
    "    axes=\"YXC\",\n",
    "    prob_thresh=0.5,  # Detection probability threshold\n",
    "    nms_thresh=0.1,  # Remove detections overlapping by more than this threshold\n",
    "    scale=1,  # Higher values are suitable for lower resolution data\n",
    "    return_labels=True,\n",
    ")\n",
    "\n",
    "# We also get detection probabilities:\n",
    "probabilities = list(polys[\"prob\"])\n",
    "\n",
    "n_detections = len(probabilities)\n",
    "\n",
    "print(f'{n_detections} cells detected.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization in Napari\n",
    "\n",
    "We use a Napari `Labels` layer to display the segmentation overlaid on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1201456626.py (8): The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model '2D_versatile_he' for 'StarDist2D'.\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.692478, nms_thresh=0.3.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a custom color lookup table based on detection probabilities\n",
    "probas_incl_bg = np.zeros(n_detections + 1)  # Include a value for the background\n",
    "probas_incl_bg[1:] = probabilities\n",
    "\n",
    "colors = plt.cm.get_cmap('inferno')(probas_incl_bg)  # Convert probability values to colors\n",
    "colors[0, -1] = 0.0  # Make the background transparent (alpha channel = 0)\n",
    "colormap = dict(zip(np.arange(len(probas_incl_bg)), colors))\n",
    "\n",
    "labels_layer = viewer.add_labels(\n",
    "    labels, \n",
    "    name='Segmentation', \n",
    "    color=colormap,\n",
    "    properties={'probabilities': probas_incl_bg},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# In Napari, you can display some text in the top-left part of the window, for example:\n",
    "viewer.text_overlay.visible = True\n",
    "viewer.text_overlay.text = f'Number of detections: {n_detections}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `stardist-napari` plugin\n",
    "\n",
    "So far in this notebook, we have only used the StarDist Python package. As it turns out, a Napari [plugin for StarDist](https://github.com/stardist/stardist-napari) also exists. If you have installed it using `pip`, you should be able to find it in the `Plugins` menu of Napari. In case you have any doubts, you can follow the instructions below to make sure you are using the plugin correctly.\n",
    "\n",
    "1. Select the Deepslides image layer.\n",
    "2. Set the model type to \"2D\".\n",
    "3. Select the \"Versatile (H&E nuclei)\" pretrained model.\n",
    "4. Check the \"Normalize Image\" checkbox.\n",
    "5. You can leave the postprocessing options as their default values. You can also try different values for the \"Probability Threshold\" and observe how it affects the number of objects detected.\n",
    "6. When the segmentation is complete, you should see that two layers have been added to the viewer. The *StarDist polygons* contains the outlines of the segmented cells as a `Shapes` layer. The *StarDist labels* contains the instance segmentation as a `Labels` layer.\n",
    "\n",
    "You can check that the nuclei segmentation produced by the plugin is similar to what you got by running the model previously in this notebook!\n",
    "\n",
    "![stardist-plugin](../images/stardist_plugin_fig.png)\n",
    "\n",
    "```{note}\n",
    "It may take a while for the StarDist plugin to first open, as it downloads some pre-trained models for you.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook, we have used `StarDist` to segment cell nuclei in an image from the DeepSlides dataset. We have seen that StarDist can be run directly from Python or from the `stardist-napari` plugin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
